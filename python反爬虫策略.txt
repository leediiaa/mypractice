反爬虫策略：

	有的网站会限制请求头，即Request Headers，那我们就去填写user-agent声明自己的身份，有时还要去填写origin和referer声明请求的来源。
	有的网站会限制登录，不登录就不给你访问。那我们就用cookies和session的知识去模拟登录。
	有的网站会做一些复杂的交互，比如设置“验证码”来阻拦登录。这就比较难做，解决方案一般有二：我们用Selenium去手动输入验证码；我们用一些图像处理的库自动识别验证码（tesserocr/pytesserart/pillow）。
	有的网站会做IP限制，什么意思呢？我们平时上网，都会有携带一个IP地址。IP地址就好像电话号码（地址码）：有了某人的电话号码，你就能与他通话了。同样，有了某个设备的IP地址，你就能与这个设备通信。
	使用搜索引擎搜索“IP”，你也能看到自己的IP地址。

	解决方案有二：使用time.sleep()来对爬虫的速度进行限制；建立IP代理池（你可以在网络上搜索可用的IP代理），一个IP不能用了就换一个用。大致语法是这样：
		import requests
		url = 'https://…'
		proxies = {'http':'http://…'}
		# ip地址
		response = requests.get(url,proxies=proxies)



解决思路：确认目标-分析过程-先面向过程一行行实现代码-代码封装

我们提出了爬虫四步：获取数据-解析数据-提取数据-存储数据。